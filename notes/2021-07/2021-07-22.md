# 2021-07-22
Carrying on implementing batch learning.  Although I have not really implemented anything yet...
I think the simplest way to implement batching is in the `_evaluate` method.
One way would be to pass X and y as parameters to the method, but the pool.map method does not allow additional parameters.  Also passing a dataframe and a series each time would be expensive (does it do that anyway?  How do I find out?).
A better way would be to pass a slice or range as a parameter.  I could then extract the subset in the evaluate method.  I suspect that a slice would be more efficient, but probably not by much in the grand scheme of things.
As I suspected - the slice time is pretty constant, and the range time is probably linear with the length of the range.  However it is still measured in microseconds so will have no impact in the grand scheme of things.
Also the syntax is the same either way - `df.iloc[x]` where x is a slice or range, so does not make much difference to the implementation.
I have written the function to generate the slices.  
Next... should the additional loop be in the ea_with_elitism_and_replacement function, or should that be called multiple times?
I think the latter, but the slice will need to be passed through, maybe as a generic context object for the evaluator.  

The code works and is better, but still doesn't converge past an accuracy of 0.5.
Also it looks strange since it does N passes on batch before moving on to the next.  It should be the other way round, do a full pass for each epoch but learning after each batch. 
... now up to 0.67 accuracy with 5 generations.
It took 12m 4s to complete 5-fold CV.

Played with the parameters - more rules but shorter.  Still does not converge, took 28m 18s.



