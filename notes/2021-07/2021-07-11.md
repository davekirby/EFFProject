# 2021-07-11
I am now in a position where I can start experimenting and tuning.
What do I want to get out of the experiments?
- a feel for how the different hyperparameters affect the outcome, in particular
    - accuracy
    - speed of convergence
    - size of best ruleset
    - average size
    - overall speed

I need to choose a decent dataset to run experiments on:
- not too big, not too small - maybe ~1000 rows?
- moderate number of features - <20?
- well known so lots of benchmarks available for other algorithms

Change the fitness to maximise the accuracy instead of minimise error?  Feels more natural, and will fit in better with RL application.  

Tried running the iris script in a loop - failed with:
Exception: Ephemerals with different functions should be named differently, even between psets.

Wow!  I drastically reduced the tree size parameters and it greatly improved convergence rate as well as size.  Ran more than twice as fast too.

I added outputting the confusion matrix to the console, but not added it to tensorboard yet.  It is pretty good results - here is a good run with an error of 4%:
```
Best Rule:  size = 10  
IF petal_width[medium] THEN [versicolor[likely], virginica[likely]]  
IF petal_length[long] THEN [setosa[unlikely], virginica[likely]]  
IF sepal_length[v.long] THEN [virginica[likely], versicolor[unlikely]]  
Final length of rules sets {3: 50}  
[[49  1  0]  
[ 0 46  4]  
[ 0  1 49]]
```
