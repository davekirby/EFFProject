# 2021-08-15
Damn, tried running gymrunner against the pendulum and it did not perform well with the new changes.  Also the best value did not change which suggests it is still not being re-evaluated.   Strange.
Reviewed the code and am pretty sure it is being re-evaluated though.  Maybe the env is restarting in the same state each time, so it is performing the same.   But then when I 'play' it afterwards it goes through the same code, so should be the same.
Can I write a unit test to check all this?  Maybe, but I need to get on with the report...

Objective for today:
- [ ] write initial notes on what I want to cover in each section
- [ ] watch training on report writing?

I am tagging report notes with #report to make it easier to manage them.

- [ ] #task make classifier sklearn compatible
    - [ ] add tests for it


**16:56**  I am puzzling as to why the lunar lander rules never do anything - they always output 0.  At least from my brief test run.
... after further investigation, I think it is because for the short training time, the best action is to not fire any engines and end the game as quickly as possible.
Maybe it would learn better if it trained for longer and with a bigger pop.
Training the LunarLanderContinuous for 50 generations x pop of 100 did much better.    Also bumped up the number of rules which I think helped too.

**21:08** Noticed that the stats always shows "best size" as 0.  I wonder why that is.







